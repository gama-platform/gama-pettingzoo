# Pac Man - Environnement multi-agents avec GAMA-PettingZoo

Version multi-agents du cÃ©lÃ¨bre jeu Pac-Man implÃ©mentÃ©e avec GAMA-PettingZoo. Cet environnement permet d'entraÃ®ner des agents dans un jeu de coopÃ©ration/compÃ©tition complexe.

## ğŸ¯ Objectif

- **Pac-Man** : Collecter tous les points tout en Ã©vitant les fantÃ´mes
- **FantÃ´mes** : Capturer Pac-Man ou l'empÃªcher de collecter les points
- **CoopÃ©ration/CompÃ©tition** : Apprentissage de stratÃ©gies multi-agents

## ğŸ“‹ Description de l'environnement

### Agents
- **1 Pac-Man** : Agent principal collecteur
- **2-4 FantÃ´mes** : Agents antagonistes
- **Interactions** : Capture, Ã©vitement, poursuite

### MÃ©caniques de jeu
- **Labyrinthe** : Navigation dans un maze avec murs
- **Points** : Collecte pour gagner des rÃ©compenses
- **Power-ups** : Bonus temporaires (optionnel)
- **Collision** : Pac-Man vs FantÃ´mes dÃ©termine la fin

## ğŸš€ DÃ©marrage rapide

### PrÃ©requis

1. **GAMA Platform** installÃ© et configurÃ©
2. **Python 3.8+** avec dÃ©pendances :
   ```bash
   pip install gama-pettingzoo numpy matplotlib stable-baselines3
   ```

### Lancement

1. **DÃ©marrer GAMA** en mode serveur :
   ```bash
   # Linux/MacOS
   ./gama-headless.sh -socket 1001
   
   # Windows
   gama-headless.bat -socket 1001
   ```

2. **Lancer le jeu** :
   ```bash
   python pacman_petz.py
   ```

## ğŸ“ Structure des fichiers

```text
Pac Man/
â”œâ”€â”€ PacMan.gaml            # ModÃ¨le GAMA principal du jeu
â”œâ”€â”€ pacman_petz.py         # Script Python d'entraÃ®nement/jeu
â””â”€â”€ README.md             # Cette documentation

# GÃ©nÃ©rÃ©s lors de l'entraÃ®nement :
trained_models/
â”œâ”€â”€ pacman_*.zip          # ModÃ¨les Pac-Man entraÃ®nÃ©s
â”œâ”€â”€ ghost_*.zip           # ModÃ¨les fantÃ´mes entraÃ®nÃ©s
â””â”€â”€ metrics/              # MÃ©triques d'entraÃ®nement
```

## âš™ï¸ Configuration

### ParamÃ¨tres GAMA (PacMan.gaml)

- **`maze_size`** : Dimensions du labyrinthe
- **`num_ghosts`** : Nombre de fantÃ´mes (2-4)
- **`num_dots`** : Nombre de points Ã  collecter
- **`ghost_speed`** : Vitesse relative des fantÃ´mes

### ParamÃ¨tres Python (pacman_petz.py)

- **Algorithmes** : PPO, DQN, MADDPG
- **Episodes d'entraÃ®nement** : 1000-5000
- **StratÃ©gies** : CoopÃ©ratives ou compÃ©titives

## ğŸ® Modes de jeu

### 1. Mode Test/Visualisation
```python
python pacman_petz.py --mode test --render
```

### 2. Mode EntraÃ®nement
```python
python pacman_petz.py --mode train --episodes 2000
```

### 3. Mode Ã‰valuation
```python
python pacman_petz.py --mode eval --models-dir ./trained_models
```

## ğŸ¤– Algorithmes d'apprentissage

### RecommandÃ©s pour cet environnement

1. **PPO (Proximal Policy Optimization)**
   - **Usage** : EntraÃ®nement stable pour tous les agents
   - **Avantages** : Convergence robuste, hyperparamÃ¨tres faciles

2. **MADDPG (Multi-Agent DDPG)**
   - **Usage** : Environnements compÃ©titifs complexes
   - **Avantages** : Gestion des agents multiples, politiques continues

3. **Independent Q-Learning**
   - **Usage** : Apprentissage simple et rapide
   - **Avantages** : Facile Ã  implÃ©menter, bon pour dÃ©buter

## ğŸ“Š Espaces d'actions et observations

### Actions (Discrete)
- **0** : Reste immobile
- **1** : Haut
- **2** : Bas  
- **3** : Gauche
- **4** : Droite

### Observations (Box)
Pour chaque agent :
- **Position propre** : (x, y)
- **Positions autres agents** : [(x1, y1), (x2, y2), ...]
- **Points visibles** : Positions des points proches
- **Murs dÃ©tectÃ©s** : Information de navigation

## ğŸ† StratÃ©gies d'entraÃ®nement

### Pac-Man
- **RÃ©compenses** : +1 par point collectÃ©, +10 si tous collectÃ©s, -10 si capturÃ©
- **StratÃ©gie** : Navigation efficace, Ã©vitement des fantÃ´mes
- **DifficultÃ©s** : Ã‰quilibrer collecte et survie

### FantÃ´mes
- **RÃ©compenses** : +10 si Pac-Man capturÃ©, -1 par point collectÃ© par Pac-Man
- **StratÃ©gie** : Coordination, encerclement, prÃ©diction
- **DifficultÃ©s** : Ã‰viter les blocages, coopÃ©ration

## ğŸ“ˆ MÃ©triques de performance

### MÃ©triques de jeu
- **Score Pac-Man** : Points collectÃ©s par partie
- **Taux de capture** : Victoires des fantÃ´mes (%)
- **DurÃ©e des parties** : Nombre de steps moyen
- **EfficacitÃ©** : Score/temps

### MÃ©triques d'apprentissage
- **RÃ©compense cumulative** par agent
- **Taux de convergence** des algorithmes
- **StabilitÃ©** des politiques

## ğŸ”§ Personnalisation

### Modifier la difficultÃ©
1. **Ajuster le nombre de fantÃ´mes** dans PacMan.gaml
2. **Changer la vitesse** des agents
3. **Modifier les rÃ©compenses** dans le contrÃ´leur
4. **Ajouter des power-ups** ou mÃ©caniques spÃ©ciales

### Nouvelles variantes
- **Pac-Man coopÃ©ratif** : Plusieurs Pac-Man
- **FantÃ´mes avec rÃ´les** : Chasseur, bloqueur, patrouilleur
- **Labyrinthe dynamique** : Murs qui changent
- **Mode survie** : Temps limitÃ©

## ğŸ› DÃ©pannage

### ProblÃ¨mes frÃ©quents

1. **"Agents bloquÃ©s dans les murs"**
   - VÃ©rifiez la logique de navigation dans PacMan.gaml
   - Ajustez les rÃ¨gles de mouvement

2. **"Apprentissage trÃ¨s lent"**
   - RÃ©duisez la complexitÃ© du labyrinthe
   - Ajustez les hyperparamÃ¨tres d'apprentissage
   - Utilisez des rÃ©compenses intermÃ©diaires

3. **"FantÃ´mes ne coopÃ¨rent pas"**
   - ImplÃ©mentez des rÃ©compenses partagÃ©es
   - Utilisez MADDPG au lieu de DQN indÃ©pendant
   - Ajoutez de la communication entre agents

## ğŸ¯ Exercices suggÃ©rÃ©s

### Niveau dÃ©butant
1. **Pac-Man simple** : Un seul fantÃ´me, petit labyrinthe
2. **Navigation de base** : Collecter les points sans fantÃ´mes
3. **Ã‰vitement simple** : FantÃ´me avec mouvement prÃ©visible

### Niveau intermÃ©diaire
1. **Multi-fantÃ´mes** : 2-3 fantÃ´mes avec stratÃ©gies diffÃ©rentes
2. **Optimisation de chemin** : Plus court chemin vers tous les points
3. **AdaptabilitÃ©** : FantÃ´mes qui apprennent les patterns de Pac-Man

### Niveau avancÃ©
1. **Coordination complexe** : Encerclement coordonnÃ©
2. **MÃ©ta-apprentissage** : Adaptation rapide Ã  nouveaux labyrinthes
3. **Communication** : Ã‰change d'informations entre fantÃ´mes

## ğŸ”— Ressources

- [Pac-Man AI Research](https://inst.eecs.berkeley.edu/~cs188/sp19/project2.html)
- [Multi-Agent RL Algorithms](https://github.com/oxwhirl/smac)
- [MADDPG Paper](https://arxiv.org/abs/1706.02275)

## ğŸ¤ Contribution

AmÃ©liorations bienvenues :
- Nouveaux algorithmes d'entraÃ®nement
- Variantes de jeu intÃ©ressantes
- MÃ©triques d'Ã©valuation avancÃ©es
- Visualisations en temps rÃ©el

---

ğŸ® **Conseil** : Commencez par entraÃ®ner Pac-Man seul, puis ajoutez progressivement les fantÃ´mes !