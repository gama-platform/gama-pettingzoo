# EntraÃ®nement d'agents dans Prison Escape avec GAMA

Ce projet permet d'entraÃ®ner deux agents IA (prisoner et guard) dans l'environnement Prison Escape de GAMA en utilisant l'apprentissage par renforcement avec l'algorithme PPO (Proximal Policy Optimization).

## ğŸ¯ Objectif

- **Prisoner** : Doit atteindre la case d'Ã©vasion (brune) sans se faire capturer
- **Guard** : Doit capturer le prisonnier avant qu'il n'atteigne la sortie

## ğŸ“‹ PrÃ©requis

1. **GAMA Platform** installÃ© et fonctionnel
2. **Python 3.8+** avec conda/miniconda
3. **Environnement gama-pettingzoo** configurÃ©

## ğŸš€ Installation rapide

1. Installer les dÃ©pendances :
```bash
python run_training.py install
```

Ou manuellement :
```bash
pip install -r requirements_training.txt
```

2. VÃ©rifier que GAMA est en marche sur le port 1001 (par dÃ©faut)

## ğŸ’» Utilisation

### EntraÃ®nement des agents

```bash
# EntraÃ®nement standard (1000 Ã©pisodes)
python run_training.py train

# EntraÃ®nement rapide pour tests (50 Ã©pisodes)
python run_training.py train --config quick

# EntraÃ®nement intensif (5000 Ã©pisodes)
python run_training.py train --config intensive

# EntraÃ®nement personnalisÃ©
python run_training.py train --episodes 500 --save-dir ./my_models --port 1002
```

### Ã‰valuation des agents

```bash
# Ã‰valuation standard (100 Ã©pisodes)
python run_training.py eval

# Ã‰valuation avec paramÃ¨tres personnalisÃ©s
python run_training.py eval --episodes 50 --render 10 --models-dir ./my_models

# Ã‰valuer un modÃ¨le spÃ©cifique
python run_training.py eval --episode-number 500
```

### Informations sur les modÃ¨les

```bash
# Afficher les modÃ¨les disponibles
python run_training.py info

# Avec un rÃ©pertoire personnalisÃ©
python run_training.py info --models-dir ./my_models
```

## ğŸ“ Structure des fichiers

```
Prison Escape/
â”œâ”€â”€ controler.gaml              # ContrÃ´leur GAMA pour PettingZoo
â”œâ”€â”€ PrisonEscape.gaml          # ModÃ¨le GAMA principal
â”œâ”€â”€ prison_petz.py             # Script de test basique
â”œâ”€â”€ train_prison_escape.py     # Module d'entraÃ®nement principal
â”œâ”€â”€ evaluate_agents.py         # Module d'Ã©valuation
â”œâ”€â”€ config.py                  # Configurations d'entraÃ®nement
â”œâ”€â”€ run_training.py            # Script utilitaire principal
â”œâ”€â”€ requirements_training.txt   # DÃ©pendances Python
â””â”€â”€ README.md                  # Ce fichier

# GÃ©nÃ©rÃ©s lors de l'entraÃ®nement :
trained_models/
â”œâ”€â”€ prisoner_ppo_episode_*.zip          # ModÃ¨les entraÃ®nÃ©s prisoner
â”œâ”€â”€ guard_ppo_episode_*.zip             # ModÃ¨les entraÃ®nÃ©s guard
â”œâ”€â”€ training_metrics_episode_*.pkl      # MÃ©triques d'entraÃ®nement
â””â”€â”€ training_progress_episode_*.png     # Graphiques de progression
```

## âš™ï¸ Configuration

### Configurations prÃ©dÃ©finies

- **default** : Configuration standard (1000 Ã©pisodes)
- **quick** : Tests rapides (50 Ã©pisodes)
- **intensive** : EntraÃ®nement approfondi (5000 Ã©pisodes)

### ParamÃ¨tres principaux

| ParamÃ¨tre | Description | Valeur par dÃ©faut |
|-----------|-------------|-------------------|
| `NUM_EPISODES` | Nombre total d'Ã©pisodes | 1000 |
| `MAX_STEPS_PER_EPISODE` | Ã‰tapes max par Ã©pisode | 200 |
| `SAVE_INTERVAL` | Sauvegarde tous les N Ã©pisodes | 100 |
| `LOG_INTERVAL` | Affichage stats tous les N Ã©pisodes | 10 |
| `GAMA_PORT` | Port de connexion GAMA | 1001 |

### ParamÃ¨tres PPO

| Agent | Learning Rate | Batch Size | Epochs | Gamma |
|-------|---------------|------------|--------|-------|
| Prisoner | 3e-4 | 64 | 10 | 0.99 |
| Guard | 3e-4 | 64 | 10 | 0.99 |

## ğŸ“Š MÃ©triques et visualisation

L'entraÃ®nement gÃ©nÃ¨re automatiquement :

1. **RÃ©compenses moyennes** par agent
2. **Taux de victoire** (prisoner vs guard)
3. **Longueur des Ã©pisodes**
4. **Progression de l'apprentissage**

Les graphiques sont sauvegardÃ©s dans le rÃ©pertoire des modÃ¨les.

## ğŸ”§ Architecture technique

### Environnement
- **Grille** : 7x7 cases
- **Actions** : 4 directions (gauche, droite, haut, bas)
- **Observations** : Positions du prisoner, guard et sortie
- **RÃ©compenses** :
  - Prisoner : +1 si Ã©vasion, -1 si capture
  - Guard : +1 si capture, -1 si Ã©vasion

### Algorithme d'apprentissage
- **PPO** (Proximal Policy Optimization)
- **Politique** : MLP (Multi-Layer Perceptron)
- **EntraÃ®nement alternÃ©** des deux agents

## ğŸ› DÃ©pannage

### Erreurs communes

1. **"Impossible de se connecter Ã  GAMA"**
   - VÃ©rifiez que GAMA est lancÃ©
   - VÃ©rifiez le port (dÃ©faut : 1001)
   - Utilisez `--port` pour changer le port

2. **"Module stable_baselines3 introuvable"**
   - ExÃ©cutez : `python run_training.py install`
   - Ou : `pip install stable-baselines3`

3. **"Erreur lors du chargement des modÃ¨les"**
   - VÃ©rifiez que l'entraÃ®nement s'est terminÃ© correctement
   - Utilisez : `python run_training.py info` pour voir les modÃ¨les disponibles

### Performance

- **GPU recommandÃ©** pour l'entraÃ®nement intensif
- **RAM** : Minimum 8GB, recommandÃ© 16GB
- **Temps d'entraÃ®nement** : ~2-6h selon la configuration

## ğŸ“ˆ Exemple d'utilisation complÃ¨te

```bash
# 1. Installation
python run_training.py install

# 2. Test rapide
python run_training.py train --config quick

# 3. VÃ©rification des rÃ©sultats
python run_training.py info

# 4. Ã‰valuation
python run_training.py eval --episodes 20 --render 3

# 5. EntraÃ®nement complet
python run_training.py train --config default

# 6. Ã‰valuation finale
python run_training.py eval --episodes 100
```

## ğŸ”„ Workflow d'entraÃ®nement

1. **Initialisation** : CrÃ©ation des modÃ¨les PPO pour chaque agent
2. **Collecte** : ExÃ©cution d'Ã©pisodes et collecte des donnÃ©es
3. **Apprentissage** : Mise Ã  jour des politiques avec PPO
4. **Ã‰valuation** : Test pÃ©riodique des performances
5. **Sauvegarde** : Sauvegarde des modÃ¨les et mÃ©triques

## ğŸ“ Notes importantes

- Les agents s'entraÃ®nent de maniÃ¨re **alternÃ©e**
- Les **rÃ©compenses sont nulles** sauf en fin d'Ã©pisode
- L'entraÃ®nement peut Ãªtre **interrompu** et **repris**
- Les **graphiques** sont mis Ã  jour automatiquement

## ğŸ¤ Contribution

Pour amÃ©liorer le systÃ¨me :
1. Modifier les paramÃ¨tres dans `config.py`
2. Ajuster les rÃ©compenses dans `controler.gaml`
3. ExpÃ©rimenter avec d'autres algorithmes dans `train_prison_escape.py`

## ğŸ“„ Licence

Ce projet fait partie du framework GAMA et suit la mÃªme licence.
