# Prison Escape - Multi-Agent Training Environment# EntraÃ®nement d'agents dans Prison Escape avec GAMA



A multi-agent reinforcement learning environment implemented with GAMA-PettingZoo. Train two competing agents (prisoner and guard) in a strategic escape scenario using advanced RL algorithms.Ce projet permet d'entraÃ®ner deux agents IA (prisoner et guard) dans l'environnement Prison Escape de GAMA en utilisant l'apprentissage par renforcement avec l'algorithme PPO (Proximal Policy Optimization).



## ğŸ¯ Objective## ğŸ¯ Objectif



- **Prisoner**: Reach the escape cell (brown) without being captured- **Prisoner** : Doit atteindre la case d'Ã©vasion (brune) sans se faire capturer

- **Guard**: Capture the prisoner before they reach the exit- **Guard** : Doit capturer le prisonnier avant qu'il n'atteigne la sortie



## ğŸ“‹ Prerequisites## ğŸ“‹ PrÃ©requis



1. **GAMA Platform** installed and running1. **GAMA Platform** installÃ© et fonctionnel

2. **Python 3.8+** with pip/conda2. **Python 3.8+** avec conda/miniconda

3. **GAMA-PettingZoo** environment configured3. **Environnement gama-pettingzoo** configurÃ©



## ğŸš€ Quick Start## ğŸš€ Installation rapide



### Installation1. Installer les dÃ©pendances :

```bash

```bashpython run_training.py install

# Install dependencies```

pip install -r requirements.txt

Ou manuellement :

# Or install core dependencies only```bash

pip install stable-baselines3 torch gymnasium matplotlib numpypip install -r requirements_training.txt

``````



### Launch GAMA Server2. VÃ©rifier que GAMA est en marche sur le port 1001 (par dÃ©faut)



```bash## ğŸ’» Utilisation

# Linux/macOS

./gama-headless.sh -socket 1001### EntraÃ®nement des agents



# Windows  ```bash

gama-headless.bat -socket 1001# EntraÃ®nement standard (1000 Ã©pisodes)

```python run_training.py train



### Basic Training# EntraÃ®nement rapide pour tests (50 Ã©pisodes)

python run_training.py train --config quick

```bash

# Standard training (1000 episodes)# EntraÃ®nement intensif (5000 Ã©pisodes)

python run_training.py trainpython run_training.py train --config intensive



# Quick test (50 episodes)# EntraÃ®nement personnalisÃ©

python run_training.py train --config quickpython run_training.py train --episodes 500 --save-dir ./my_models --port 1002

```

# Custom training

python run_training.py train --episodes 500 --save-dir ./my_models### Ã‰valuation des agents

```

```bash

## ğŸ“ Project Structure# Ã‰valuation standard (100 Ã©pisodes)

python run_training.py eval

```text

Prison Escape/# Ã‰valuation avec paramÃ¨tres personnalisÃ©s

â”œâ”€â”€ ğŸ“„ README.md                    # This documentationpython run_training.py eval --episodes 50 --render 10 --models-dir ./my_models

â”œâ”€â”€ ğŸ“„ requirements.txt             # Python dependencies

â”œâ”€â”€ ğŸ“„ config.py                    # Training configurations# Ã‰valuer un modÃ¨le spÃ©cifique

â”œâ”€â”€ ğŸ“„ run_training.py              # Main training scriptpython run_training.py eval --episode-number 500

â”œâ”€â”€ ğŸ“„ train_prison_escape.py       # PPO training implementation  ```

â”œâ”€â”€ ğŸ“„ improved_train_prison_escape.py  # Q-Learning with anti-static mechanisms

â”œâ”€â”€ ğŸ“„ evaluate_agents.py           # Agent evaluation module### Informations sur les modÃ¨les

â”œâ”€â”€ ğŸ“„ improved_evaluate_agents.py  # Enhanced evaluation with metrics

â”œâ”€â”€ ğŸ“„ test_environment.py          # Environment testing utilities```bash

â”œâ”€â”€ ğŸ“„ image_viewer.py              # Real-time visualization tool# Afficher les modÃ¨les disponibles

â”œâ”€â”€ ğŸ“„ controler.gaml               # GAMA PettingZoo controllerpython run_training.py info

â”œâ”€â”€ ğŸ“„ PrisonEscape.gaml            # Main GAMA simulation model

â””â”€â”€ ğŸ“ snapshot/                    # Generated visualization frames# Avec un rÃ©pertoire personnalisÃ©

python run_training.py info --models-dir ./my_models

# Generated during training:```

trained_models/

â”œâ”€â”€ prisoner_ppo_episode_*.zip      # Trained prisoner models## ğŸ“ Structure des fichiers

â”œâ”€â”€ guard_ppo_episode_*.zip         # Trained guard models  

â”œâ”€â”€ training_metrics_*.pkl          # Training metrics data```

â””â”€â”€ training_progress_*.png         # Progress visualizationPrison Escape/

```â”œâ”€â”€ controler.gaml              # ContrÃ´leur GAMA pour PettingZoo

â”œâ”€â”€ PrisonEscape.gaml          # ModÃ¨le GAMA principal

## ğŸ¤– Training Algorithmsâ”œâ”€â”€ prison_petz.py             # Script de test basique

â”œâ”€â”€ train_prison_escape.py     # Module d'entraÃ®nement principal

### PPO (Proximal Policy Optimization)â”œâ”€â”€ evaluate_agents.py         # Module d'Ã©valuation

- **File**: `train_prison_escape.py`â”œâ”€â”€ config.py                  # Configurations d'entraÃ®nement

- **Features**: Stable, reliable training for both agentsâ”œâ”€â”€ run_training.py            # Script utilitaire principal

- **Best for**: Standard training scenariosâ”œâ”€â”€ requirements_training.txt   # DÃ©pendances Python

â””â”€â”€ README.md                  # Ce fichier

### Improved Q-Learning

- **File**: `improved_train_prison_escape.py` # GÃ©nÃ©rÃ©s lors de l'entraÃ®nement :

- **Features**: Anti-static mechanisms, curriculum learning, diversity bonusestrained_models/

- **Best for**: Avoiding convergence to static strategiesâ”œâ”€â”€ prisoner_ppo_episode_*.zip          # ModÃ¨les entraÃ®nÃ©s prisoner

â”œâ”€â”€ guard_ppo_episode_*.zip             # ModÃ¨les entraÃ®nÃ©s guard

## ğŸ’» Usage Examplesâ”œâ”€â”€ training_metrics_episode_*.pkl      # MÃ©triques d'entraÃ®nement

â””â”€â”€ training_progress_episode_*.png     # Graphiques de progression

### Training Commands```



```bash## âš™ï¸ Configuration

# PPO training with different configurations

python run_training.py train --config default     # 1000 episodes### Configurations prÃ©dÃ©finies

python run_training.py train --config quick       # 50 episodes  

python run_training.py train --config intensive   # 5000 episodes- **default** : Configuration standard (1000 Ã©pisodes)

- **quick** : Tests rapides (50 Ã©pisodes)

# Improved Q-Learning training- **intensive** : EntraÃ®nement approfondi (5000 Ã©pisodes)

python improved_train_prison_escape.py

### ParamÃ¨tres principaux

# Custom parameters

python run_training.py train --episodes 2000 --port 1002| ParamÃ¨tre | Description | Valeur par dÃ©faut |

```|-----------|-------------|-------------------|

| `NUM_EPISODES` | Nombre total d'Ã©pisodes | 1000 |

### Evaluation Commands| `MAX_STEPS_PER_EPISODE` | Ã‰tapes max par Ã©pisode | 200 |

| `SAVE_INTERVAL` | Sauvegarde tous les N Ã©pisodes | 100 |

```bash| `LOG_INTERVAL` | Affichage stats tous les N Ã©pisodes | 10 |

# Evaluate trained models| `GAMA_PORT` | Port de connexion GAMA | 1001 |

python run_training.py eval --episodes 100

### ParamÃ¨tres PPO

# Enhanced evaluation with visualizations

python improved_evaluate_agents.py| Agent | Learning Rate | Batch Size | Epochs | Gamma |

|-------|---------------|------------|--------|-------|

# Evaluate specific models| Prisoner | 3e-4 | 64 | 10 | 0.99 |

python run_training.py eval --models-dir ./my_models --render 5| Guard | 3e-4 | 64 | 10 | 0.99 |

```

## ğŸ“Š MÃ©triques et visualisation

### Visualization

L'entraÃ®nement gÃ©nÃ¨re automatiquement :

```bash

# Real-time environment visualization1. **RÃ©compenses moyennes** par agent

python image_viewer.py2. **Taux de victoire** (prisoner vs guard)

3. **Longueur des Ã©pisodes**

# View training progress4. **Progression de l'apprentissage**

python run_training.py info --models-dir ./trained_models

```Les graphiques sont sauvegardÃ©s dans le rÃ©pertoire des modÃ¨les.



## âš™ï¸ Configuration## ğŸ”§ Architecture technique



### Main Parameters### Environnement

- **Grille** : 7x7 cases

| Parameter | Description | Default |- **Actions** : 4 directions (gauche, droite, haut, bas)

|-----------|-------------|---------|- **Observations** : Positions du prisoner, guard et sortie

| `NUM_EPISODES` | Total training episodes | 1000 |- **RÃ©compenses** :

| `MAX_STEPS_PER_EPISODE` | Max steps per episode | 200 |  - Prisoner : +1 si Ã©vasion, -1 si capture

| `SAVE_INTERVAL` | Save models every N episodes | 100 |  - Guard : +1 si capture, -1 si Ã©vasion

| `LOG_INTERVAL` | Display stats every N episodes | 10 |

| `GAMA_PORT` | GAMA connection port | 1001 |### Algorithme d'apprentissage

- **PPO** (Proximal Policy Optimization)

### Algorithm Settings- **Politique** : MLP (Multi-Layer Perceptron)

- **EntraÃ®nement alternÃ©** des deux agents

#### PPO Configuration

- **Learning Rate**: 3e-4## ğŸ› DÃ©pannage

- **Batch Size**: 64  

- **Training Epochs**: 10### Erreurs communes

- **Discount Factor**: 0.99

1. **"Impossible de se connecter Ã  GAMA"**

#### Q-Learning Configuration     - VÃ©rifiez que GAMA est lancÃ©

- **Learning Rate**: 0.1   - VÃ©rifiez le port (dÃ©faut : 1001)

- **Epsilon**: 0.3 â†’ 0.05 (adaptive)   - Utilisez `--port` pour changer le port

- **Epsilon Decay**: 0.9995

- **Discount Factor**: 0.952. **"Module stable_baselines3 introuvable"**

   - ExÃ©cutez : `python run_training.py install`

## ğŸ® Environment Details   - Ou : `pip install stable-baselines3`



### Grid Layout3. **"Erreur lors du chargement des modÃ¨les"**

- **Size**: 7x7 grid   - VÃ©rifiez que l'entraÃ®nement s'est terminÃ© correctement

- **Prisoner Start**: Top-left corner   - Utilisez : `python run_training.py info` pour voir les modÃ¨les disponibles

- **Guard Start**: Center of grid

- **Escape Cell**: Bottom-right corner (brown)### Performance



### Action Space- **GPU recommandÃ©** pour l'entraÃ®nement intensif

- **0**: Stay in place- **RAM** : Minimum 8GB, recommandÃ© 16GB

- **1**: Move up- **Temps d'entraÃ®nement** : ~2-6h selon la configuration

- **2**: Move down  

- **3**: Move left## ğŸ“ˆ Exemple d'utilisation complÃ¨te

- **4**: Move right

```bash

### Observation Space# 1. Installation

For each agent:python run_training.py install

- Own position (x, y)

- Other agent position (x, y) # 2. Test rapide

- Escape cell position (x, y)python run_training.py train --config quick

- Grid boundaries and obstacles

# 3. VÃ©rification des rÃ©sultats

### Reward Structurepython run_training.py info

- **Prisoner**: +1 for escape, -1 for capture, 0 otherwise

- **Guard**: +1 for capture, -1 for escape, 0 otherwise# 4. Ã‰valuation

python run_training.py eval --episodes 20 --render 3

## ğŸ“Š Training Metrics

# 5. EntraÃ®nement complet

### Tracked Metricspython run_training.py train --config default

- **Episode Rewards**: Cumulative rewards per agent

- **Win Rates**: Success percentage for each agent# 6. Ã‰valuation finale

- **Episode Lengths**: Steps until terminationpython run_training.py eval --episodes 100

- **Learning Progress**: Policy improvement over time```



### Visualization Features## ğŸ”„ Workflow d'entraÃ®nement

- Real-time training progress plots

- Win rate evolution charts1. **Initialisation** : CrÃ©ation des modÃ¨les PPO pour chaque agent

- Episode length distributions2. **Collecte** : ExÃ©cution d'Ã©pisodes et collecte des donnÃ©es

- Agent movement heatmaps3. **Apprentissage** : Mise Ã  jour des politiques avec PPO

4. **Ã‰valuation** : Test pÃ©riodique des performances

## ğŸ”§ Advanced Features5. **Sauvegarde** : Sauvegarde des modÃ¨les et mÃ©triques



### Anti-Static Mechanisms (Improved Q-Learning)## ğŸ“ Notes importantes

- **Static Behavior Detection**: Identifies repetitive patterns

- **Dynamic Penalties**: Punishes static strategies- Les agents s'entraÃ®nent de maniÃ¨re **alternÃ©e**

- **Exploration Bonuses**: Rewards diverse actions- Les **rÃ©compenses sont nulles** sauf en fin d'Ã©pisode

- **Curriculum Learning**: Adaptive episode lengths- L'entraÃ®nement peut Ãªtre **interrompu** et **repris**

- Les **graphiques** sont mis Ã  jour automatiquement

### Enhanced Evaluation

- **Statistical Analysis**: Comprehensive performance metrics## ğŸ¤ Contribution

- **Behavior Analysis**: Movement pattern detection

- **Strategy Visualization**: Action frequency heatmapsPour amÃ©liorer le systÃ¨me :

- **Performance Comparisons**: Multi-model evaluation1. Modifier les paramÃ¨tres dans `config.py`

2. Ajuster les rÃ©compenses dans `controler.gaml`

## ğŸ› Troubleshooting3. ExpÃ©rimenter avec d'autres algorithmes dans `train_prison_escape.py`



### Common Issues## ğŸ“„ Licence



1. **"Cannot connect to GAMA"**Ce projet fait partie du framework GAMA et suit la mÃªme licence.

   - Ensure GAMA is running on correct port
   - Check firewall settings
   - Verify GAMA model loads correctly

2. **"Module not found" errors**  
   - Install dependencies: `pip install -r requirements.txt`
   - Check Python environment activation

3. **Training very slow**
   - Use GPU if available (PyTorch CUDA)
   - Reduce episode count for testing
   - Check system resources (RAM/CPU)

4. **Models not saving**
   - Check directory permissions
   - Ensure sufficient disk space
   - Verify save path exists

### Performance Optimization
- **GPU Training**: Install PyTorch with CUDA support
- **Memory**: Minimum 8GB RAM, recommended 16GB
- **Training Time**: 30min-4h depending on configuration

## ğŸ“ˆ Example Workflow

```bash
# 1. Environment setup
pip install -r requirements.txt
./gama-headless.sh -socket 1001

# 2. Quick test training
python run_training.py train --config quick

# 3. Check results  
python run_training.py info

# 4. Full training
python improved_train_prison_escape.py  # or
python run_training.py train --config default

# 5. Comprehensive evaluation
python improved_evaluate_agents.py

# 6. Visualize performance
python image_viewer.py
```

## ğŸ§ª Experimentation

### Hyperparameter Tuning
Modify parameters in `config.py`:
- Learning rates
- Network architectures  
- Reward structures
- Training schedules

### Algorithm Comparison
Compare different approaches:
- PPO vs Q-Learning performance
- Static vs anti-static strategies
- Various exploration strategies

### Environment Modifications
Extend the environment:
- Larger grid sizes
- Multiple prisoners/guards
- Dynamic obstacles
- Power-ups and items

## ğŸ¤ Contributing

To improve the training system:

1. **Fork** the repository
2. **Modify** algorithms in training files
3. **Adjust** rewards in `controler.gaml`
4. **Test** with different configurations
5. **Submit** pull request with improvements

### Development Guidelines
- Follow PEP 8 coding standards
- Add comprehensive docstrings
- Include unit tests for new features
- Document algorithm modifications

## ğŸ“š References

- [Stable-Baselines3 Documentation](https://stable-baselines3.readthedocs.io/)
- [GAMA Platform](https://gama-platform.org/)
- [PettingZoo Multi-Agent Environments](https://pettingzoo.farama.org/)
- [PPO Algorithm Paper](https://arxiv.org/abs/1707.06347)

## ğŸ“„ License

This project is part of the GAMA platform and follows the same MIT license.

---

ğŸ® **Tip**: Start with quick training to verify setup, then move to full training for best results!